{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import json\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n",
    "from utils import round_float, is_nan\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = \"<|BOS|>\"\n",
    "START = \"<start>\"\n",
    "END = \"<end>\"\n",
    "\n",
    "\n",
    "SEPARATOR_TOKENS = [\n",
    "    BOS_TOKEN,\n",
    "    START,\n",
    "    END\n",
    "]\n",
    "\n",
    "LINE_TOKEN = \"<line>\"\n",
    "VERTICAL_BAR_TOKEN = \"<vertical_bar>\"\n",
    "HORIZONTAL_BAR_TOKEN = \"<horizontal_bar>\"\n",
    "SCATTER_TOKEN = \"<scatter>\"\n",
    "DOT_TOKEN = \"<dot>\"\n",
    "\n",
    "CHART_TYPE_TOKENS = [\n",
    "    LINE_TOKEN,\n",
    "    VERTICAL_BAR_TOKEN,\n",
    "    HORIZONTAL_BAR_TOKEN,\n",
    "    SCATTER_TOKEN,\n",
    "    DOT_TOKEN,\n",
    "]\n",
    "\n",
    "new_tokens = SEPARATOR_TOKENS + CHART_TYPE_TOKENS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_series_to_string(json_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        json_dict (Dict[str, Any]): ターゲットのdict\n",
    "    Returns:\n",
    "        gt_string (str): 入力となるプロンプト\n",
    "    \"\"\"\n",
    "    all_x, all_y = [], []\n",
    "\n",
    "    for d in json_dict['data-series']:\n",
    "        x = d[\"x\"]\n",
    "        y = d[\"y\"]\n",
    "\n",
    "        x = round_float(x)\n",
    "        y = round_float(y)\n",
    "\n",
    "        # Ignore nan values\n",
    "        if is_nan(x) or is_nan(y):\n",
    "            continue\n",
    "\n",
    "        all_x.append(x)\n",
    "        all_y.append(y)\n",
    "\n",
    "    chart_type = f\"<{json_dict['chart-type']}>\"\n",
    "    data_str = \\\n",
    "        START + \\\n",
    "        '|'.join([f'{x}|{y}' for x, y in zip(all_x, all_y)]) \\\n",
    "        + END\n",
    "\n",
    "    gt_string = BOS_TOKEN + chart_type + data_str\n",
    "\n",
    "    return gt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained('google/matcha-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45631d5a68a04119ba25c42fb38b1179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60578 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_lengthes = []\n",
    "chart_types = []\n",
    "sources = []\n",
    "\n",
    "env = lmdb.open('../../../data/data0004/lmdb', max_readers=32,\n",
    "                readonly=True, lock=False, readahead=False, meminit=False)\n",
    "with env.begin(write=False) as txn:\n",
    "    n_samples = int(txn.get('num-samples'.encode()))\n",
    "    for idx in trange(n_samples):\n",
    "        # load json\n",
    "        label_key = f'label-{str(idx+1).zfill(8)}'.encode()\n",
    "        label = txn.get(label_key).decode('utf-8')\n",
    "        json_dict = json.loads(label)\n",
    "        data_str = data_series_to_string(json_dict)\n",
    "        token_ids = processor.tokenizer.encode(data_str)\n",
    "        token_lengthes.append(len(token_ids))\n",
    "        chart_types.append(json_dict['chart-type'])\n",
    "        sources.append(json_dict['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "longs = [len > 512 for len in token_lengthes]\n",
    "long_source = [source for i, source in enumerate(sources) if longs[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'generated': 224, 'extracted': 17})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(long_source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_tokens = []\n",
    "unk_token_id = processor.tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a9ff1a5fc7438e84b11ffcff1725d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60578 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = lmdb.open('../../../data/data0004/lmdb', max_readers=32,\n",
    "                readonly=True, lock=False, readahead=False, meminit=False)\n",
    "with env.begin(write=False) as txn:\n",
    "    n_samples = int(txn.get('num-samples'.encode()))\n",
    "    for idx in trange(n_samples):\n",
    "        # load json\n",
    "        label_key = f'label-{str(idx+1).zfill(8)}'.encode()\n",
    "        label = txn.get(label_key).decode('utf-8')\n",
    "        json_dict = json.loads(label)\n",
    "        data_str = data_series_to_string(json_dict)\n",
    "        token_ids = processor.tokenizer.encode(data_str)\n",
    "        tokens = processor.tokenizer.tokenize(data_str)\n",
    "        for token, token_id in zip(tokens, token_ids):\n",
    "            if token_id == unk_token_id:\n",
    "                unk_tokens.append(token)\n",
    "    unk_counter = Counter(unk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'<unk>': 157, '\\n': 284, '\\n\\n': 28, 'ދ': 2})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decide separate character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[273, 324, 1]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.encode(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c263c05cdd34274ae9e472a3d8d1e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60578 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_token_id = 324 # processor.tokenizer.encode(',')\n",
    "check_tokens = []\n",
    "\n",
    "with env.begin(write=False) as txn:\n",
    "    n_samples = int(txn.get('num-samples'.encode()))\n",
    "    for idx in trange(n_samples):\n",
    "        # load json\n",
    "        label_key = f'label-{str(idx+1).zfill(8)}'.encode()\n",
    "        label = txn.get(label_key).decode('utf-8')\n",
    "        json_dict = json.loads(label)\n",
    "        data_str = data_series_to_string(json_dict)\n",
    "\n",
    "        token_ids = processor.tokenizer.encode(data_str)\n",
    "        tokens = processor.tokenizer.tokenize(data_str)\n",
    "        for token, token_id in zip(tokens, token_ids):\n",
    "            if token_id == check_token_id:\n",
    "                check_tokens.append(token)\n",
    "                break\n",
    "    check_counter = Counter(check_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[273, 285, 275, 289, 1]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer('').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Would grant Gov't/Dail too much control\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 0), match=''>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('|', \"Would grant Gov't/Dail too much control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(re.search('|', '324'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|' in '324|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mga-RYrVM4UM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
